{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE FACE READER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.init\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import zipfile\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Face Reader/Left.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b9ce3d89716a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlab1data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Face Reader/Left.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlab1data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlab1data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# data 압축 풀기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Face Reader/Left.zip'"
     ]
    }
   ],
   "source": [
    "lab1data = zipfile.ZipFile('./Face Reader/Left.zip')\n",
    "lab1data.extractall('.')\n",
    "lab1data.close()  # data 압축 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        \n",
    "        self.layer1=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3,48,3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(48),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "\n",
    "            torch.nn.Conv2d(48,96,3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(96),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(96*4*4,120),\n",
    "            torch.nn.ReLU(),            \n",
    "            torch.nn.Linear(120,84),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(84,3)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        x=x.view(x.size()[0],-1)\n",
    "        x=self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왼쪽 눈에 대한 classifiacation training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1536, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=84, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:41: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:     1] loss  = 0.97534\n",
      "[Epoch:     2] loss  = 0.76688\n",
      "[Epoch:     3] loss  = 0.65389\n",
      "[Epoch:     4] loss  = 0.59468\n",
      "[Epoch:     5] loss  = 0.47507\n",
      "[Epoch:     6] loss  = 0.36195\n",
      "[Epoch:     7] loss  = 0.29494\n",
      "[Epoch:     8] loss  = 0.23742\n",
      "[Epoch:     9] loss  = 0.2195\n",
      "[Epoch:    10] loss  = 0.13985\n",
      "[Epoch:    11] loss  = 0.11341\n",
      "[Epoch:    12] loss  = 0.087638\n",
      "[Epoch:    13] loss  = 0.10734\n",
      "[Epoch:    14] loss  = 0.095866\n",
      "[Epoch:    15] loss  = 0.091915\n",
      "[Epoch:    16] loss  = 0.048565\n",
      "[Epoch:    17] loss  = 0.085083\n",
      "[Epoch:    18] loss  = 0.070853\n",
      "[Epoch:    19] loss  = 0.038736\n",
      "[Epoch:    20] loss  = 0.037821\n",
      "[Epoch:    21] loss  = 0.00937\n",
      "[Epoch:    22] loss  = 0.0042479\n",
      "[Epoch:    23] loss  = 0.0091408\n",
      "[Epoch:    24] loss  = 0.005125\n",
      "[Epoch:    25] loss  = 0.0012429\n",
      "[Epoch:    26] loss  = 0.00076907\n",
      "[Epoch:    27] loss  = 0.00069834\n",
      "[Epoch:    28] loss  = 0.00055301\n",
      "[Epoch:    29] loss  = 0.00043254\n",
      "[Epoch:    30] loss  = 0.00049017\n",
      "[Epoch:    31] loss  = 0.00048411\n",
      "[Epoch:    32] loss  = 0.00041391\n",
      "[Epoch:    33] loss  = 0.00031631\n",
      "[Epoch:    34] loss  = 0.00022735\n",
      "[Epoch:    35] loss  = 0.00023387\n",
      "[Epoch:    36] loss  = 0.00022665\n",
      "[Epoch:    37] loss  = 0.00019617\n",
      "[Epoch:    38] loss  = 0.00020214\n",
      "[Epoch:    39] loss  = 0.00020479\n",
      "[Epoch:    40] loss  = 0.00016939\n",
      "[Epoch:    41] loss  = 0.00016523\n",
      "[Epoch:    42] loss  = 0.00013399\n",
      "[Epoch:    43] loss  = 0.00011926\n",
      "[Epoch:    44] loss  = 0.00011406\n",
      "[Epoch:    45] loss  = 0.00010449\n",
      "[Epoch:    46] loss  = 0.00010439\n",
      "[Epoch:    47] loss  = 0.00010652\n",
      "[Epoch:    48] loss  = 0.00010508\n",
      "[Epoch:    49] loss  = 8.4297e-05\n",
      "[Epoch:    50] loss  = 7.4501e-05\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # transform 설정\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root='./Face Reader/train_Left_eyes',transform=transform) # train set 설정\n",
    "\n",
    "batch_size = 16   # batch_size : 16 설정\n",
    "\n",
    "data_loader =  torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "classes1 = ('down','middle','up') # 0 : down, 1 : middle, 2 : up\n",
    "\n",
    "\n",
    "# LeNet으로 모델 설정\n",
    "left_eye = LeNet()\n",
    "print(left_eye)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "training_epochs = 50\n",
    "optimizer = torch.optim.Adam(left_eye.parameters(), lr=.0005) \n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0\n",
    "    total_batch = len(trainset) // batch_size \n",
    "    \n",
    "    for i, (batch_images, batch_labels) in enumerate(data_loader):\n",
    "        \n",
    "        X = Variable(batch_images)\n",
    "        Y = Variable(batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        E = left_eye(X)\n",
    "        loss = loss_func(E, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss / total_batch\n",
    "        \n",
    "    print(\"[Epoch: {:>5}] loss  = {:>.5}\".format(epoch+1, avg_loss.data[0]))\n",
    " \n",
    "torch.save(left_eye.state_dict(), './Left_eye.path')  # 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오른쪽 눈에 대한 classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab1data = zipfile.ZipFile('./Face Reader/Right.zip')\n",
    "lab1data.extractall('.')\n",
    "lab1data.close()  # data 압축 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:35: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:     1] loss  = 0.97451\n",
      "[Epoch:     2] loss  = 0.74746\n",
      "[Epoch:     3] loss  = 0.64498\n",
      "[Epoch:     4] loss  = 0.58153\n",
      "[Epoch:     5] loss  = 0.49984\n",
      "[Epoch:     6] loss  = 0.43611\n",
      "[Epoch:     7] loss  = 0.36253\n",
      "[Epoch:     8] loss  = 0.28043\n",
      "[Epoch:     9] loss  = 0.24357\n",
      "[Epoch:    10] loss  = 0.24405\n",
      "[Epoch:    11] loss  = 0.12769\n",
      "[Epoch:    12] loss  = 0.10586\n",
      "[Epoch:    13] loss  = 0.098186\n",
      "[Epoch:    14] loss  = 0.071215\n",
      "[Epoch:    15] loss  = 0.049296\n",
      "[Epoch:    16] loss  = 0.1072\n",
      "[Epoch:    17] loss  = 0.10819\n",
      "[Epoch:    18] loss  = 0.077199\n",
      "[Epoch:    19] loss  = 0.024151\n",
      "[Epoch:    20] loss  = 0.028928\n",
      "[Epoch:    21] loss  = 0.016131\n",
      "[Epoch:    22] loss  = 0.043756\n",
      "[Epoch:    23] loss  = 0.040858\n",
      "[Epoch:    24] loss  = 0.013387\n",
      "[Epoch:    25] loss  = 0.021991\n",
      "[Epoch:    26] loss  = 0.03568\n",
      "[Epoch:    27] loss  = 0.11121\n",
      "[Epoch:    28] loss  = 0.04916\n",
      "[Epoch:    29] loss  = 0.032618\n",
      "[Epoch:    30] loss  = 0.046794\n",
      "[Epoch:    31] loss  = 0.032075\n",
      "[Epoch:    32] loss  = 0.022253\n",
      "[Epoch:    33] loss  = 0.0058409\n",
      "[Epoch:    34] loss  = 0.0029297\n",
      "[Epoch:    35] loss  = 0.0013086\n",
      "[Epoch:    36] loss  = 0.0006784\n",
      "[Epoch:    37] loss  = 0.00053994\n",
      "[Epoch:    38] loss  = 0.00050557\n",
      "[Epoch:    39] loss  = 0.00033882\n",
      "[Epoch:    40] loss  = 0.00031369\n",
      "[Epoch:    41] loss  = 0.00032543\n",
      "[Epoch:    42] loss  = 0.00024209\n",
      "[Epoch:    43] loss  = 0.00021582\n",
      "[Epoch:    44] loss  = 0.00021715\n",
      "[Epoch:    45] loss  = 0.0001993\n",
      "[Epoch:    46] loss  = 0.00018291\n",
      "[Epoch:    47] loss  = 0.00018644\n",
      "[Epoch:    48] loss  = 0.00014649\n",
      "[Epoch:    49] loss  = 0.00014581\n",
      "[Epoch:    50] loss  = 0.00011757\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.ImageFolder(root='./Face Reader/train_Right_eyes',transform=transform) # train set 설정\n",
    "\n",
    "batch_size = 16   # batch_size : 16 설정\n",
    "\n",
    "data_loader =  torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "classes2 = ('down','middle','up') # 0 : down, 1 : middle, 2 : up\n",
    "\n",
    "# LeNet으로 모델 설정\n",
    "right_eye = LeNet()\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "training_epochs = 50\n",
    "optimizer = torch.optim.Adam(right_eye.parameters(), lr=.0005) \n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0\n",
    "    total_batch = len(trainset) // batch_size \n",
    "    \n",
    "    for i, (batch_images, batch_labels) in enumerate(data_loader):\n",
    "        \n",
    "        X = Variable(batch_images)\n",
    "        Y = Variable(batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        E = right_eye(X)\n",
    "        loss = loss_func(E, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss / total_batch\n",
    "        \n",
    "    print(\"[Epoch: {:>5}] loss  = {:>.5}\".format(epoch+1, avg_loss.data[0]))\n",
    "\n",
    "    \n",
    "torch.save(right_eye.state_dict(), './right_eye.path')  # 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 눈의 크기에 대한 classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab1data = zipfile.ZipFile('./Face Reader/Size.zip')\n",
    "lab1data.extractall('.')\n",
    "lab1data.close()  # data 압축 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:     1] loss  = 0.52665\n",
      "[Epoch:     2] loss  = 0.43287\n",
      "[Epoch:     3] loss  = 0.39339\n",
      "[Epoch:     4] loss  = 0.33821\n",
      "[Epoch:     5] loss  = 0.29109\n",
      "[Epoch:     6] loss  = 0.26944\n",
      "[Epoch:     7] loss  = 0.2161\n",
      "[Epoch:     8] loss  = 0.17158\n",
      "[Epoch:     9] loss  = 0.15418\n",
      "[Epoch:    10] loss  = 0.11153\n",
      "[Epoch:    11] loss  = 0.084419\n",
      "[Epoch:    12] loss  = 0.0584\n",
      "[Epoch:    13] loss  = 0.092789\n",
      "[Epoch:    14] loss  = 0.061003\n",
      "[Epoch:    15] loss  = 0.059472\n",
      "[Epoch:    16] loss  = 0.037647\n",
      "[Epoch:    17] loss  = 0.021522\n",
      "[Epoch:    18] loss  = 0.0057167\n",
      "[Epoch:    19] loss  = 0.020605\n",
      "[Epoch:    20] loss  = 0.0294\n",
      "[Epoch:    21] loss  = 0.038362\n",
      "[Epoch:    22] loss  = 0.040381\n",
      "[Epoch:    23] loss  = 0.0419\n",
      "[Epoch:    24] loss  = 0.032882\n",
      "[Epoch:    25] loss  = 0.036944\n",
      "[Epoch:    26] loss  = 0.046116\n",
      "[Epoch:    27] loss  = 0.015896\n",
      "[Epoch:    28] loss  = 0.0055567\n",
      "[Epoch:    29] loss  = 0.0019648\n",
      "[Epoch:    30] loss  = 0.00096676\n",
      "[Epoch:    31] loss  = 0.00075097\n",
      "[Epoch:    32] loss  = 0.0036566\n",
      "[Epoch:    33] loss  = 0.042169\n",
      "[Epoch:    34] loss  = 0.036544\n",
      "[Epoch:    35] loss  = 0.023551\n",
      "[Epoch:    36] loss  = 0.026094\n",
      "[Epoch:    37] loss  = 0.023519\n",
      "[Epoch:    38] loss  = 0.020511\n",
      "[Epoch:    39] loss  = 0.023138\n",
      "[Epoch:    40] loss  = 0.0089828\n",
      "[Epoch:    41] loss  = 0.0064275\n",
      "[Epoch:    42] loss  = 0.0010393\n",
      "[Epoch:    43] loss  = 0.0038307\n",
      "[Epoch:    44] loss  = 0.056292\n",
      "[Epoch:    45] loss  = 0.022182\n",
      "[Epoch:    46] loss  = 0.01698\n",
      "[Epoch:    47] loss  = 0.0044217\n",
      "[Epoch:    48] loss  = 0.011304\n",
      "[Epoch:    49] loss  = 0.01763\n",
      "[Epoch:    50] loss  = 0.024152\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])  # transform 설정\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root='./Face Reader/train_size',transform=transform) # train set 설정\n",
    "\n",
    "batch_size = 16   # batch_size : 16 설정\n",
    "\n",
    "data_loader =  torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "classes3 = ('big', 'small') # 0 : big, 1 : small\n",
    "\n",
    "# LeNet으로 모델 설정\n",
    "size_eye = LeNet()\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "training_epochs = 50\n",
    "optimizer = torch.optim.Adam(size_eye.parameters(), lr=.0005) \n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0\n",
    "    total_batch = len(trainset) // batch_size \n",
    "    \n",
    "    for i, (batch_images, batch_labels) in enumerate(data_loader):\n",
    "        \n",
    "        X = Variable(batch_images)\n",
    "        Y = Variable(batch_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        E = size_eye(X)\n",
    "        loss = loss_func(E, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss / total_batch\n",
    "        \n",
    "    print(\"[Epoch: {:>5}] loss  = {:>.5}\".format(epoch+1, avg_loss.data[0]))\n",
    "\n",
    "torch.save(size_eye.state_dict(), './size_eye.path')   # 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 관상 데이터들 분류시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 눈이 클 때\n",
    "\n",
    "L1  = \"감정표현이 뛰어나다.\"\n",
    "L2 = \"감정 중시하며, 천진하고, 착하다.\"\n",
    "L3 = \"동정심 많음, 금전이나 애정 문제로 남에게 쉽게 이용당할 수 있다.\"\n",
    "L4 = \"애정에서는 우유부단하고 주저하여 결정을 내리지 못하는 경우가 있고, 심지어 양다리를 걸치는 상황도 생길 수 있다.\"\n",
    "L5 = \"시야가 넓고 명랑하고 외향적이며 사교와 단체 생활을 좋아한다.\"\n",
    "L6 = \"관찰력이 예리하고 반응이 민첩하다.\"\n",
    "L7 = \"색채 분별력이 뛰어나고 음악이나 회화 쪽으로 재능을 발휘할 수 있다.\"\n",
    "L8 = \"목표를 이루기 위한 의지와 집중력이 부족하기 때문에 전문 분야로 성과를 거두기 어려울 수 있다.\"\n",
    "L9 = \"언변이 좋아 이성의 환심을 살 수 있다.\"\n",
    "L10 = \"마음이 열려 있어 정이 많고, 열정적이다.\"\n",
    "L11 = \"호기심이 넘치고 개방적인 성격을 갖추고 있다.\"\n",
    "L12 = \"정이 많아 이성에 대한 관심과 인기도도 많고 개방적인 성격을 갖고 있다.\"\n",
    "L13 = \"적극적인 애정공세를 펴는 경우가 많다.\"\n",
    "L14 = \"심리 변화가 심하기 때문에 즉흥적인 행동을 보여 오해를 받는 경우가 많다.\"\n",
    "L15 = \"현실보다는 이상을 추구하여 금전적으로 기복이 심하다.\"\n",
    "L16 = \"일반적으로 얼굴을 보았을 때 크다고 느껴지는 눈을 가진 사람은 감각이 뛰어나고 이성을 끌어들이는 매력이 있으며 개방적이다.\"\n",
    "L17 = \"정열적인 성격을 갖추고 있으며 상대방을 잘 배려해주는 한편, 상대방의 마음을 읽어내는 재능이 있다.\"\n",
    "L18 = \"개방적인 성격이기는 하지만, 사람을 가려서 사귀는 편이고 정열이 지나치게 강해서 애정문제에 빠지면 헤어나지 못한다.\"\n",
    "L19 = \"사랑을 할 때에는 최선을 다 하지만, 사랑이 식으면 미련 없이 등을 돌리는 냉정함이 있다.\"\n",
    "L20 = \"남성의 경우에는 리더가 될 수 있는 자질을 충분히 갖추고 있기 때문에 다른 사람 밑에서 일하는 것에 거부감을 느낀다. 단, 직장생활을 하면 승진이 빠른 편이다.\"\n",
    "L21 = \"여성의 경우에는 남성에게 인기가 좋으며 음악적 감각이 뛰어나서 노래를 잘하며 춤에도 소질이 있다.\"\n",
    "\n",
    "L = [L1, L2, L3, L4, L5, L6, L7, L8, L9, L10, L11, L12, L13, L14, L15, L16, L17, L18, L19, L20, L21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 눈이 작을 때\n",
    "\n",
    "S1 = \"차분하고 겸손한 성격을 갖추고 있다.\"\n",
    "S2 = \"강인하고 냉정한 자기만의 세계를 가진 사람이 많다.\"\n",
    "S3 = \"말보다는 행동으로 생각을 표현하는 신중함을 가진다.\"\n",
    "S4 = \"자신의 속내를 쉽게 드러내지 않는다.\"\n",
    "S5 = \"사회적으로 믿음직하다는 평가를 받는다.\"\n",
    "S6 = \"한번 마음먹은 일은 가능하면 끝까지 성사시키려는 끈기도 있다.\"\n",
    "S7 = \"힘든 시기가 닥치더라도 꿋꿋이 이겨낼 수 있는 사람이다.\"\n",
    "S8 = \"젊은 시절에 고생이 많고 매력이 뒤떨어져 윗사람들의 사랑을 받지 못한다.\"\n",
    "S9 = \"겸손한 성격으로 대인관계에서 자신을 굽힐 줄 알고 지적인 능력이 뛰어나기 때문에 학문적인 분야에서 성공할 가능성이 높다.\"\n",
    "S10 = \"특히 한 우물을 파서 성공을 거두는 예가 많지만, 성격이 매우 강해 냉정하다는 인상을 주기 쉽고 자신만의 공간에 틀어박혀 좀처럼 마음을 열지 않는다.\"\n",
    "S11 = \"남성의 경우 여자를 다루는 능력과 금전을 융통하는 능력은 부족하지만, 믿음직하고 성실하기 때문에 늦게 인정을 받는 타입이다.\"\n",
    "S12 = \"의지가 강하기 때문에 난관을 잘 극복한다.\"\n",
    "S13 = \"여성의 경우에는 남성을 선택하는 데 많은 시간이 걸리지만, 한번 마음을 주면 어지간해서는 다른 이성에게 눈길을 돌리지 않는 일편단심형이며 가족을 매우 중요하게 생각한다.\"\n",
    "S14 = \"가정 경제를 꾸려나가는 능력이 있고 사회활동을 해도 성공할 수 있다.\"\n",
    "\n",
    "S = [S1, S2, S3, S4, S5, S6, S7, S8, S9, S10, S11, S12, S13, S14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 눈꼬리가 올라간 눈\n",
    "U1 = \"성급하며 양의 특성 상 기질이 강하고 빠르고 폭발적이고 급한 것이다.\"\n",
    "U2 = \"감각이 뛰어나고 어떤 일에도 굽히지 않는 강한 용기를 갖추고 있으며 두뇌회전이 빠르고 기회를 잡는 능력이 뛰어나다.\"\n",
    "U3 = \"예술적인 방면에 소질이 있고 추친력을 갖추고 있으며 아무리 어려운 난관에 부딪혀도 강한 인내력으로 돌파할 수 있는 용기가 있다.\"\n",
    "U4 = \"기회가 오면 어떻게 해서든 움켜쥐려하기 때문에 이기적이라는 인상을 주기 쉽고 독단적인 성향이 강하다.\"\n",
    "U5 = \"남성의 경우에는 두뇌회전이 빨라 중간관리직으로 잘 어울리며 실행력이 있어 운세가 좋은 편이다.\"\n",
    "U6 = \"자신의 주장을 약간 억제하고 다른 사람의 의견을 받아들이는 포용력을 갖추는 것이 바람직하다.\"\n",
    "U7 = \"성공을 추구하는 눈이다.\"\n",
    "U8 = \"성격이 예민하고 반응이 빠르고 결단력이 있고 시기를 놓치지 않는다.\"\n",
    "U9 = \"그러나 자존심과 승부욕 소유욕이 강하고 의심이 많은 것이 단점이다.\"\n",
    "U10 = \"품격이 있다.\"\n",
    "U11 = \"두뇌 회전이 빠르고 총명하다.\"\n",
    "U12 = \"예상 밖의 아이디어를 가져 영리해 보일 수 있다.\"\n",
    "U13 = \"남의 어려움을 앞장 서 해결하므로 인복이 많다.\"\n",
    "U14 = \"애정 문제에서 주도권을 잡고 적극적으로 어필한다.\"\n",
    "U15 = \"점유욕과 지배욕이 있다.\"\n",
    "U16 = \"끈기가 있고 체력이 강하다.\"\n",
    "U17 = \"주관이 분명하고 대범한 성격을 갖췄다.\"\n",
    "U18 = \"어떤 일을 하든 반드시 성사시키는 강인함을 갖추고 있다.\"\n",
    "U19 = \"리더십도 매우 뛰어나 ‘대인의 상’, ‘장수의 상’이라고 표현한다.\"\n",
    "U20 = \"자존심도 강해 다른 사람에게 지는 것을 싫어한다.\"\n",
    "U21 = \"자신의 영역을 침범당하면 즉각 자기방어에 나설 정도로 철저한 자기관리 능력을 자랑한다.\"\n",
    "\n",
    "U = [U1, U2, U3, U4, U5, U6, U7, U8, U9, U10, U11, U12, U13, U14, U15, U16, U17, U18, U19, U20, U21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 눈꼬리가 내려간 눈\n",
    "\n",
    "O1 = \"만약 반대로 눈끝이 아래로 숙인 자는 음에 속하니 문질이며 부드럽고 약하며 침착하며 느린 것이다.\"\n",
    "O2 = \"눈꼬리가 처진 눈을 가진 사람은 심리적으로 느긋하고 여유 있는 성격이며 투쟁이나 다툼보다는 평화를 사랑한다.\"\n",
    "O3 = \"모든 일을 긍정적이고 원만하게 처리하려 하기 때문에 대인관계가 매우 좋아 다른 사람의 도움으로 출세를 할 가능성이 매우 높다. \"\n",
    "O4 = \"성실하다는 점도 장점이다.\"\n",
    "O5 = \"수동적이며 소극적이기 때문에 주위 사람들로부터 자신의 주장을 할 줄 모르는 사람이라는 비난을 받는다.\"\n",
    "O6 = \"남성의 경우에는 친구나 동료, 선후배와의 관계가 원만해서 일찍 출세할 수 있다.\"\n",
    "O7 = \"여성의 유혹에 넘어가기 쉽고 그 때문에 실패를 맛볼 가능성이 높다.\"\n",
    "O8 = \"여성의 경우에는 역시 남성의 유혹에 넘어가기 쉽고 그 때문에 손해를 볼 가능성이 높다.\"\n",
    "O9 = \"사교적이며 인정이 많다.\"\n",
    "O10 = \"인정이 많고 보스 기질이 있다.\"\n",
    "O11 = \"대인관계가 좋고 주변에 사람이 많이 모이는 편이다.\"\n",
    "O12 = \"유머도 풍부하여 재미있고 즐거운 인생을 보낼 것 같으나 사실 외로움도 많이 탄다.\"\n",
    "O13 = \"이성에 대한 호기심이 매우 강해서 정 때문에 마음을 졸일 가능성이 매우 높다.\"\n",
    "O14 = \"모든 사람에게 친절하고 다정하게 행동하지만, 그에 못지 않을 정도로 자존심이 강하고 보스 기질이 강하기 때문에 실질적으로는 성격이 매우 강한 사람이다.\"\n",
    "\n",
    "O = [O1, O2, O3, O4, O5, O6, O7, O8, O9, O10, O11, O12, O13, O14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 눈꼬리가 일직선으로 수평인 눈\n",
    "M1 = \"불상불하의 눈으로 불투(사물을 훔쳐보지 말아야)해야 모름지기 쓸 만한 그릇이 된다.\"\n",
    "M2 = \"위인(爲人)이 강개롭고 심평정직하다.\"\n",
    "M3 = \"위, 아래로 향하지 않고 수평을 유지하는 것이 가장 이상적이다.\"\n",
    "\n",
    "M = [M1, M2, M3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 짝짝이 눈\n",
    "\n",
    "D1 = \"성격이 변덕스럽고 우유부단하다.\"\n",
    "D2 = \"부모님의 사이가 좋지않을 확률이 높다.\"\n",
    "D3 = \"성격상 소극적이면서 어두운 면이 있다.\"\n",
    "D4 = \"남다른 관찰력과 예민한 직감력을 지녔다.\"\n",
    "D5 = \"인생 굴곡이 많다.\"\n",
    "D6 = \" 활동적이고 야심이 있고 부를 축적한다.\"\n",
    "D7 = \"세상을 두가지 관점으로 보는 경향이 있어 객관성이 매우 뛰어나고 논리적이다.\"\n",
    "D8 = \"어떤 분야에서든 상위에까지 오르기는 하지만, 최상위에 오르기는 어려움이 있다.\"\n",
    "D9 = \"주변에 시기와 질투를 하는 사람들이 많다.\"\n",
    "D10 = \"한쪽은 크고 한쪽은 작은 눈을 가진 사람은 인생에서 큰 전환기를 겪을 가능성이 높고 두뇌회전이 빠른 편이다.\"\n",
    "D11 = \"자기 주장이 뚜렷하고 활동적이며 승부에 대한 열정이 강하고 이상도 높다.\"\n",
    "D12 = \"고집이 세고 자기 주장이 강해 견제의 대상이 될 가능성이 높고 이성에게 약한 편이며 인생에 기복이 아주 심하다.\"\n",
    "D13 = \"남성의 경우에는 왼쪽이 클 경우에는 매우 활동적이고 승부욕이 강하며 이상이 높고, 오른쪽이 클 경우에는 정에 이끌리기는 해도 리더심과 자신감이 있어서 노력에 따라 행복을 만끽할 수 있다.\"\n",
    "\n",
    "D = [D1, D2, D3, D4, D5, D6, D7, D8, D9, D10, D11, D12, D13, D14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자가 직접 돌릴 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자의 사진을 받아 얼굴 인식하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # 얼굴 인식을 위한 opencv 설치\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './picture/1.jpg'\n",
    "cascadefile = \"./Face Reader/haarcascade_lefteye_2splits.xml\"  # 왼쪽 눈을 인식\n",
    "cascadefile1 = \"./Face Reader/haarcascade_righteye_2splits.xml\" # 오른쪽 눈을 따로 인식\n",
    "img = cv2.imread(filename) \n",
    "imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # 컬러 이미지를 인식할 수 있는 흑백으로 바꿔 줌\n",
    "\n",
    "\n",
    "# 왼쪽 눈에 대해 왼쪽 눈 부분만 추출\n",
    "cascade = cv2.CascadeClassifier(cascadefile)\n",
    "facelist = cascade.detectMultiScale(imgray, scaleFactor=2.08, minNeighbors=1) \n",
    "\n",
    "cropped = []\n",
    "if len(facelist) >= 1: \n",
    "    for face in facelist: \n",
    "        x, y, w, h = face \n",
    "        cv2.rectangle(imgray, (x, y), (x+w, y+h), (255, 0, 0), 2) # 눈에 해당하는 부위를 네모로 표시\n",
    "        cropped = imgray[y:y+h, x:x+w]  # 눈에 해당하는 부분을 추출함\n",
    "    result_filename = [\"./Face Reader/real/left/1.jpg\"]\n",
    "    result_filename = ''.join(result_filename)\n",
    "    cv2.imwrite(result_filename,cropped)   # 추출한 눈 부위를 저장\n",
    "if not np.any(cropped):     # 눈을 인식하지 못했을 때\n",
    "    print('왼쪽 눈을 인식하지 못했습니다..ㅜㅠ')  \n",
    "\n",
    "# 오른쪽 눈에 대해 오른쪽 눈 부분만 추출\n",
    "cascade = cv2.CascadeClassifier(cascadefile1)\n",
    "facelist = cascade.detectMultiScale(imgray, scaleFactor=2.08, minNeighbors=1) \n",
    "\n",
    "cropped=[]\n",
    "if len(facelist) >= 1: \n",
    "    for face in facelist: \n",
    "        x, y, w, h = face \n",
    "        cv2.rectangle(imgray, (x, y), (x+w, y+h), (255, 0, 0), 2)   # 눈에 해당하는 부위를 네모로 표시\n",
    "        cropped = imgray[y:y+h, x:x+w]   # 눈에 해당하는 부분을 추출함\n",
    "    result_filename = [\"./Face Reader/real1/right/1.jpg\"]   \n",
    "    result_filename = ''.join(result_filename)\n",
    "    cv2.imwrite(result_filename,cropped)  # 추출한 눈 부위를 저장\n",
    "if not np.any(cropped):    # 눈을 인식하지 못했을 때\n",
    "    print('오른쪽 눈을 인식하지 못했습니다..ㅠㅜ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자의 얼굴(눈)을 classification하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(24),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size=16\n",
    "testset = torchvision.datasets.ImageFolder(root='./Face Reader/real',transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True)  # 추출했던 왼쪽눈을 testset으로 설정\n",
    "\n",
    "testset1 = torchvision.datasets.ImageFolder(root='./Face Reader/real1',transform=transform)\n",
    "test_loader1 = torch.utils.data.DataLoader(dataset=testset1, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=True)  # 추출했던 오른쪽 눈을 testset으로 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_l, test_labels_l = next(iter(test_loader))   # 왼쪽 눈에 대한 이미지와 라벨 받음\n",
    "test_images_r, test_labels_r = next(iter(test_loader1))  # 오른쪽 눈에 대한 이미지와 라벨 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=84, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_eye = LeNet()   # 모델 클래스 입력\n",
    "left_eye.load_state_dict(torch.load('./Left_eye.path'))\n",
    "left_eye.eval()\n",
    "\n",
    "right_eye = LeNet()\n",
    "right_eye.load_state_dict(torch.load('./right_eye.path'))\n",
    "right_eye.eval()\n",
    "\n",
    "size_eye = LeNet()\n",
    "size_eye.load_state_dict(torch.load('./size_eye.path'))\n",
    "size_eye.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 0, 0])\n",
      "tensor([0, 0, 1, 0])\n",
      "tensor([2, 1, 2, 1])\n",
      "tensor([1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "X = Variable(test_images_l.view(-1,3,24,24).float())\n",
    "E1 = left_eye(X)   # 왼쪽 눈에 대해 up, middle, down 판정\n",
    "print(torch.max(E1,1)[1])\n",
    "\n",
    "E3 = size_eye(X)  # 눈의 크기를 판정\n",
    "print(torch.max(E3,1)[1])\n",
    "\n",
    "Y = Variable(test_images_r.view(-1,3,24,24).float())\n",
    "E2 = right_eye(Y)   # 오른 쪽 눈에 대해 up, middle, down 판정\n",
    "print(torch.max(E2,1)[1])\n",
    "\n",
    "E4 = size_eye(Y)  # 눈의 크기를 판정\n",
    "print(torch.max(E4,1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes1 = ('down','middle','up')\n",
    "classes2 = ('down','middle','up')\n",
    "classes3 = ('big', 'small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "i=0   # i를 0으로 설정하는 이유는 제일 최근에 만들어진 사진이 처음으로 들어가기 때문 - 다른 사진의 결과를 보고싶다면 i를 바꾸면 됨\n",
    "print(torch.max(E1,1)[1][i])\n",
    "print(torch.max(E2,1)[1][i])\n",
    "print(torch.max(E3,1)[1][i])\n",
    "print(torch.max(E4,1)[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "middle\n",
      "up\n",
      "big\n",
      "small\n"
     ]
    }
   ],
   "source": [
    "# 눈 크기와 눈꼬리에 대한 클래스 출력\n",
    "print(classes1[torch.max(E1,1)[1][i]])\n",
    "print(classes2[torch.max(E2,1)[1][i]])\n",
    "print(classes3[torch.max(E3,1)[1][i]])\n",
    "print(classes3[torch.max(E4,1)[1][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용자 얼굴에 대한 classification 결과로 관상 측정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 적으로 위의 관상평에서 각각 4개씩 랜덤으로 선택하여 출력시킴<br>\n",
    "(1) 양쪽 눈꼬리 결과가 같을 경우 : 각각 해당하는 눈꼬리의 관상평 4개<br>\n",
    "(2) 양쪽 눈꼬리 결과가 다를 경우 : 양쪽에서 2개씩 선택<br>\n",
    "(3) 양쪽 눈 크기 결과가 같을 경우 : 각각 해당하는 눈꼬리의 관상평 4개<br>\n",
    "(4) 양쪽 눈 크기 결과가 같을 경우 : 양쪽에서 각각 한 가지씩과 짝짝이 눈의 관상평 2개 선택<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예술적인 방면에 소질이 있고 추친력을 갖추고 있으며 아무리 어려운 난관에 부딪혀도 강한 인내력으로 돌파할 수 있는 용기가 있다.\n",
      "자신의 주장을 약간 억제하고 다른 사람의 의견을 받아들이는 포용력을 갖추는 것이 바람직하다.\n",
      "주관이 분명하고 대범한 성격을 갖췄다.\n",
      "관찰력이 예리하고 반응이 민첩하다.\n",
      "자신의 속내를 쉽게 드러내지 않는다.\n",
      "성격이 변덕스럽고 우유부단하다.\n",
      "자기 주장이 뚜렷하고 활동적이며 승부에 대한 열정이 강하고 이상도 높다.\n"
     ]
    }
   ],
   "source": [
    "# 눈꼬리\n",
    "if classes1[torch.max(E1,1)[1][i]] == classes2[torch.max(E2,1)[1][i]]:  # 양쪽 눈꼬리가 같을 때\n",
    "    if classes1[torch.max(E1,1)[1][i]] == 'down':  # 둘다 눈꼬리가 내려갔으면\n",
    "        ind = random.sample(range(14),4)\n",
    "        print(O[ind[0]]+'\\n'+O[ind[1]]+'\\n'+O[ind[2]]+'\\n'+O[ind[3]])  # 내려간 눈꼬리에 해당하는 관상 결과 출력\n",
    "    elif classes1[torch.max(E1,1)[1][i]] == 'up':   # 둘다 눈꼬리가 올라갔으면\n",
    "        ind = random.sample(range(21),4)\n",
    "        print(U[ind[0]]+'\\n'+U[ind[1]]+'\\n'+U[ind[2]]+'\\n'+U[ind[3]])  # 올라간 눈꼬리에 해당하는 관상 결과 출력\n",
    "    else:    # 둘다 눈꼬리가 수평에 이르면\n",
    "        print(M[0]+'\\n'+M[1]+'\\n'+M[2])   # 수평인 눈꼬리에 해당하는 관상 결과 출력\n",
    "elif classes1[torch.max(E1,1)[1][i]]  or classes1[torch.max(E2,1)[1][i]] == 'middle':\n",
    "    if classes1[torch.max(E1,1)[1][i]] or classes1[torch.max(E2,1)[1][i]] == 'up':  # 눈꼬리가 수평과 올라갔다면\n",
    "        ind = random.sample(range(21),3)\n",
    "        print(U[ind[0]]+'\\n'+U[ind[1]]+'\\n'+U[ind[2]])  # 적은 올라간 눈꼬리 관상 결과 출력\n",
    "    else:      # 눈꼬리가 수평과 내려갔다면\n",
    "        ind = random.sample(range(14),3)\n",
    "        print(O[ind[0]]+'\\n'+O[ind[1]]+'\\n'+O[ind[2]])  # 적은 내려간 눈꼬리 관상 결과 출력 \n",
    "else:\n",
    "    ind = random.sample(range(14),4)\n",
    "    print(O[ind[0]]+'\\n'+O[ind[1]]+'\\n'+U[ind[2]]+'\\n'+U[ind[3]])   # 양쪽의 눈꼬리가 다를 경우 양쪽의 성질 모두 출력\n",
    "    \n",
    "# 눈 크기\n",
    "if classes3[torch.max(E3,1)[1][i]] == classes3[torch.max(E4,1)[1][i]]:   # 양쪽 눈의 크기가 같으면\n",
    "    if classes3[torch.max(E3,1)[1][i]] == 'big' :   # 눈의 크기가 클 때\n",
    "        ind = random.sample(range(21),4)\n",
    "        print(L[ind[0]]+'\\n'+L[ind[1]]+'\\n'+L[ind[2]]+'\\n'+L[ind[3]])   # 큰 눈의 관상 결과를 출력\n",
    "    else:\n",
    "        ind = random.sample(range(14),4)\n",
    "        print(S[ind[0]]+'\\n'+S[ind[1]]+'\\n'+S[ind[2]]+'\\n'+S[ind[3]])  # 작은 눈의 관상 결과를 출력\n",
    "else:   # 두 눈의 결과가 다를 때\n",
    "    ind = random.sample(range(14),4)\n",
    "    print(L[ind[0]]+'\\n'+S[ind[1]]+'\\n'+D[ind[2]]+'\\n'+D[ind[3]])   # 짝짝이 눈의 관상 결과를 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "cbd00fba-81b9-4d87-9f5a-23aaec573e2c",
    "theme": {
     "cbd00fba-81b9-4d87-9f5a-23aaec573e2c": {
      "backgrounds": {
       "dc7afa04-bf90-40b1-82a5-726e3cff5267": {
        "background-color": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "id": "dc7afa04-bf90-40b1-82a5-726e3cff5267"
       }
      },
      "id": "cbd00fba-81b9-4d87-9f5a-23aaec573e2c",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         197,
         226,
         245
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "a": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c"
       },
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 8
       },
       "h2": {
        "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "font-family": "Merriweather",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "li": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-size": 3.25
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "color": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
       "font-family": "Lato",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
